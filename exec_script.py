from overall_planner import OverallPlanner
from scene_comprehension import SceneComprehension
from step_termination import TerminationCheck
from step_planner import Plan2Action
from frankapy import FrankaArm
import robomail.vision as vis
from PIL import Image
import numpy as np
import os
import pyrealsense2 as rs
import ast

home_rotation = np.array([0, 0, 0],
                         [0, 0, 0],
                         [0, 0, 0])

def rotate_x(home_pose, angle):
    """
    Rotate the home pose around the x-axis by the given angle.
    
    Parameters:
    home_pose (numpy.ndarray): The home pose 3x3 matrix.
    angle (float): The rotation angle in radians.
    
    Returns:
    numpy.ndarray: The rotated home pose 3x3 matrix.
    """
    rotation_matrix = np.array([
        [1, 0, 0],
        [0, np.cos(angle), -np.sin(angle)],
        [0, np.sin(angle), np.cos(angle)]
    ])
    return np.dot(home_pose, rotation_matrix)

def rotate_y(home_pose, angle):
    """
    Rotate the home pose around the y-axis by the given angle.
    
    Parameters:
    home_pose (numpy.ndarray): The home pose 3x3 matrix.
    angle (float): The rotation angle in radians.
    
    Returns:
    numpy.ndarray: The rotated home pose 3x3 matrix.
    """
    rotation_matrix = np.array([
        [np.cos(angle), 0, np.sin(angle)],
        [0, 1, 0],
        [-np.sin(angle), 0, np.cos(angle)]
    ])
    return np.dot(home_pose, rotation_matrix)

def rotate_z(home_pose, angle):
    """
    Rotate the home pose around the z-axis by the given angle.
    
    Parameters:
    home_pose (numpy.ndarray): The home pose 3x3 matrix.
    angle (float): The rotation angle in radians.
    
    Returns:
    numpy.ndarray: The rotated home pose 3x3 matrix.
    """
    rotation_matrix = np.array([
        [np.cos(angle), -np.sin(angle), 0],
        [np.sin(angle), np.cos(angle), 0],
        [0, 0, 1]
    ])
    return np.dot(home_pose, rotation_matrix)

def run_command(act, feature, deltas, fa):
    if act == 'go-to':
        pose = fa.get_pose()
        print("Feature: ",feature)
        print("Deltas: ", deltas)
        pose.translation = feature + deltas/100
        fa.goto_pose(pose)
        
    elif act == "grasp":
        if feature == '0':
            fa.open_gripper()
        elif feature == '1':
            fa.goto_gripper(width=0.0, grasp=True)

    elif act == "tilt":
        if features[0] != '0':
            pose = fa.get_pose()
            angle = ast.literal_eval(feature[0])
            pose.rotation = rotate_x(home_rotation, np.radians(angle)) 
        elif features[1] != '0':
            pose = fa.get_pose()
            angle = ast.literal_eval(feature[1])
            pose.rotation = rotate_y(home_rotation, np.radians(angle))
        elif features[2] != '0':
            pose = fa.get_pose()
            angle = ast.literal_eval(feature[2])
            pose.rotation = rotate_z(home_rotation, np.radians(angle)) 
    return


if __name__ == "__main__":
    #TODO:
    Task = "Place the tape measure on top of the board"

    #TODO: Change this to be generated by an LLM
    ActionList = ["Grasp", "Push-down", "Move-to", "Release", "Roll", "Scoop", "Pour"]

    #TODO:
    save_dir = "/home/arvind/LLM_Tool/LLM-Tool/Save_dir"

    fa = FrankaArm()
    fa.reset_joints()
    fa.open_gripper()

    cam1 = vis.CameraClass(cam_number=1)
    cam2 = vis.CameraClass(cam_number=2)
    cam3 = vis.CameraClass(cam_number=3)
    cam4 = vis.CameraClass(cam_number=4)
    cam5 = vis.CameraClass(cam_number=5)

    cam6 = vis.CameraClass(cam_number=6) # This is the overview camera

    W = 1280
    H = 800
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_device('152522250441')
    config.enable_stream(rs.stream.color, W, H, rs.format.bgr8, 30)
    pipeline.start(config)

    img1, _, _, _, _ = cam1.get_next_frame()
    img2, _, pc2, _, _ = cam2.get_next_frame()
    img3, _, pc3, _, _ = cam3.get_next_frame()
    img4, _, pc4, _, _ = cam4.get_next_frame()
    img5, _, pc5, _, _ = cam5.get_next_frame()

    img6, _, _, _, _ = cam6.get_next_frame()

    ImgList = [img1, img2, img3, img4, img5]
    
    save_path = save_dir + '/step0'
    os.makedirs(save_path, exist_ok=True)
    max_size = 512


    for i, img_arr in enumerate(ImgList):
        img = Image.fromarray(img_arr)
        img.thumbnail((max_size, max_size))
        img.save(save_path + f"/Image{i+1}.png")


    # Query Scene comp, get list of objects

    ObjList = SceneComprehension(save_path, Task)
    PosList = [f"original position of {obj}" for obj in ObjList]
    PosList.extend([f"{obj}" for obj in ObjList])
    print(ObjList)
    
    #TODO: Query Point-LLM, to get positions

    ObjLocList = [np.array([0.723, 0.0,  0.033]), np.array([0.536, 0.0, 0.043]), np.array([0.723, 0.0,  0.033]), np.array([0.536, 0.0, 0.043])]
    LocDict = dict(zip(PosList, ObjLocList))

    prev_steps = {}
    #Pass query and list of objects to planner 
    StepsList = OverallPlanner(Task, ObjList, PosList, ActionList)
    num_steps = len(StepsList)
    

    #Iterate through list of steps and query step planner to get got-to poses
    i=1
    while i<=num_steps:
        print(f"Executing Step {i}: {StepsList[i-1]}")
        Action = StepsList[i-1][0]
        Location = StepsList[i-1][1]
        Object = StepsList[i-1][2]
        Tool = StepsList[i-1][3]

        glob_pos = LocDict[Location]

        #Query the steps to actions LLM
        
        # TODO: Might be better to pass an image as well to the Plan2Action here (to get an idea of the current scene)
        CommandList = Plan2Action(Action, Location, Object, Tool, prev_steps)
        

        #TODO: Execute the actions   
        for command in CommandList:
            print(command)
            act = command[0]
            feature = command[1]
            deltas = None
            if act == 'go-to':
                print("Go-to Command Identified. Accessing LocDict")
                tuple_string = command[2]
                tuple_elements = tuple_string.strip('()').split(',')
                tuple_numbers = [int(element.strip().split()[0]) for element in tuple_elements]
                deltas = np.array(tuple_numbers)
                feature = LocDict[feature]
            elif act == "tilt":
                print("Tilt Command Identified.")
                tuple_string = command[1]
                tuple_elements = tuple_string.strip('()').split(',')
                tuple_numbers = [int(element.strip().split()[0]) for element in tuple_elements]
                features = np.array(tuple_numbers)
            else:
                print("Grasp Command Identified.")
            
            run_command(act, feature, deltas, fa)


        # Termination check
        img1, _, _, _, _ = cam1.get_next_frame()
        img2, _, pc2, _, _ = cam2.get_next_frame()
        img3, _, pc3, _, _ = cam3.get_next_frame()
        img4, _, pc4, _, _ = cam4.get_next_frame()
        img5, _, pc5, _, _ = cam5.get_next_frame()

        ImgList = [img1, img2, img3, img4, img5]
        save_path = save_dir + f'/step{i}'
        os.makedirs(save_path, exist_ok=True)

        for j, img_arr in enumerate(ImgList):
            img = Image.fromarray(img_arr)
            img.thumbnail((max_size, max_size))
            img.save(save_path + f"/Image{j+1}.png")

        #TODO: Query pointllm to get new positions and make a new LocDict
        # NewPos = []
        # NewPosList = [f"New Position of {obj}" for obj in NewPosList]


        if TerminationCheck(save_path, Action):
            inputs = (Action, Location, Object, Tool)
            prev_steps[inputs] = CommandList
            i+=1

        else:
            #TODO: Replan using OverallPlanner LLM
            StepsList = OverallPlanner(Task, ObjList, PosList, ActionList, StepsList, i)

